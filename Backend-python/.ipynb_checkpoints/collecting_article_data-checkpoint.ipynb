{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/sp/DIC_LAB2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page: 0...\n",
      "Processing page: 1...\n",
      "Processing page: 2...\n",
      "Processing page: 3...\n",
      "Processing page: 4...\n",
      "Processing page: 5...\n",
      "Processing page: 6...\n"
     ]
    }
   ],
   "source": [
    "import urllib3,json,datetime,time,sys,os,re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from nytimesarticle import articleAPI\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "urllib3.disable_warnings()\n",
    "api = articleAPI('980a67e3390a4f0180296f0e3bfb6303') #set API key\n",
    "def parse_articles(articles): #This function takes in a response to the NYT api  #and parses the articles into a list of dictionaries\n",
    "    news = []\n",
    "    if \"response\" in articles.keys():\n",
    "        for i in articles['response']['docs']:\n",
    "            dic = {}\n",
    "            dic['url'] = i['web_url']\n",
    "            news.append(dic)\n",
    "    else:\n",
    "        return news\n",
    "    return news\n",
    "all_articles = []\n",
    "#NYT limits pager to first 100 pages. But rarely will you find over 100 pages of results anyway.\n",
    "for j in range(0,101):\n",
    "    articles = api.search(q = 'cambridge+analytica',begin_date = 20180401,end_date = 20180408,page = j)\n",
    "    #articles = api.search(q = 'cambridge+analytica',begin_date = 20180407,end_date = 20180408,page = j)\n",
    "    time.sleep(5)\n",
    "    articles = parse_articles(articles)\n",
    "    if len(articles) == 0:\n",
    "        break\n",
    "    all_articles = all_articles + articles\n",
    "    print ('Processing page: ' + str(j) + '...')\n",
    "urls = [i[\"url\"] for i in all_articles]\n",
    "paragraphs = []\n",
    "for url in urls: #navigate to each URL and collect article data\n",
    "    paragraphs.append(\" \".join([i.text for i in BeautifulSoup(urllib3.connection_from_url(url).urlopen('GET',url).data, \"lxml\").findAll(\"p\") if i.text != \"\" and \"advertisement\" not in i.text.lower()]))    \n",
    "article_data = ''.join(paragraphs) #Coverting list to string\n",
    "with open(\"cambridge_week_NYT_data.txt\", \"w\") as text_file: #writing into a textfile\n",
    "    text_file.write(article_data)\n",
    "#http://dlab.berkeley.edu/blog/scraping-new-york-times-articles-python-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
